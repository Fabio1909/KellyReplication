{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-16T22:45:17.148193Z",
     "start_time": "2025-08-16T22:45:17.146196Z"
    }
   },
   "source": [
    "from traning_classes import DataManager, ModelPerformanceIndicators\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T22:45:19.332398Z",
     "start_time": "2025-08-16T22:45:19.330500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_names = [\n",
    "    \"I60-R5\",\n",
    "    \"I60-R20\",\n",
    "    \"I60-R60\"\n",
    "]\n",
    "\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import GlorotUniform  # Xavier initializer\n",
    "\n",
    "def I60_architecture(input_shape=(96, 180)):\n",
    "    model = models.Sequential([\n",
    "        # First Convolutional Block\n",
    "        layers.Conv2D(64, (5, 3), padding='same', kernel_initializer=GlorotUniform(),\n",
    "                      dilation_rate=(2, 1), input_shape=(*input_shape, 1)),  # Dilation only\n",
    "        layers.BatchNormalization(),  # Batch normalization\n",
    "        layers.LeakyReLU(alpha=0.1),  # Leaky ReLU with alpha=0.1\n",
    "        layers.MaxPooling2D((3, 1)),  # Simulating vertical strides\n",
    "\n",
    "        # Second Convolutional Block\n",
    "        layers.Conv2D(128, (5, 3), padding='same', kernel_initializer=GlorotUniform(),\n",
    "                      strides=(1, 1), dilation_rate=(1, 1)),  # Regular convolution\n",
    "        layers.BatchNormalization(),  # Batch normalization\n",
    "        layers.LeakyReLU(alpha=0.1),  # Leaky ReLU with alpha=0.1\n",
    "        layers.MaxPooling2D((2, 1)),  # Pooling with size (2, 1)\n",
    "\n",
    "        # Third Convolutional Block\n",
    "        layers.Conv2D(256, (5, 3), padding='same', kernel_initializer=GlorotUniform(),\n",
    "                      strides=(1, 1), dilation_rate=(1, 1)),  # Regular convolution\n",
    "        layers.BatchNormalization(),  # Batch normalization\n",
    "        layers.LeakyReLU(alpha=0.1),  # Leaky ReLU with alpha=0.1\n",
    "        layers.MaxPooling2D((2, 1)),  # Pooling with size (2, 1)\n",
    "\n",
    "        # Third Convolutional Block\n",
    "        layers.Conv2D(512, (5, 3), padding='same', kernel_initializer=GlorotUniform(),\n",
    "                      strides=(1, 1), dilation_rate=(1, 1)),  # Regular convolution\n",
    "        layers.BatchNormalization(),  # Batch normalization\n",
    "        layers.LeakyReLU(alpha=0.1),  # Leaky ReLU with alpha=0.1\n",
    "        layers.MaxPooling2D((2, 1)),  # Pooling with size (2, 1)\n",
    "\n",
    "        # Flatten and Fully Connected Layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1, kernel_initializer=GlorotUniform()),  # Fully connected layer\n",
    "        layers.Dropout(0.5),  # 50% dropout\n",
    "        layers.Activation('sigmoid')  # Binary classification (True/False)\n",
    "    ])\n",
    "\n",
    "    # Use Adam optimizer with a learning rate of 0.01\n",
    "    optimizer = Adam(learning_rate=0.01)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ],
   "id": "599f168e8e655e8f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T22:45:21.199129Z",
     "start_time": "2025-08-16T22:45:21.195130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gc\n",
    "import tensorflow as tf\n",
    "\n",
    "def train_pipeline(model_name):\n",
    "    try:\n",
    "        # === GET DATA ===\n",
    "        dm = DataManager()\n",
    "        image_dir = f\"../_images/{model_name}/traning\"\n",
    "        dm.load_images(image_dir)\n",
    "        X_train, X_test, y_train, y_test = dm.split_data()\n",
    "\n",
    "        # === GET MODEL ARCHITECTURE ===\n",
    "        model = I60_architecture()\n",
    "\n",
    "        # === TRAINING ===\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.1,\n",
    "            patience=2,\n",
    "            min_lr=1e-6\n",
    "        )\n",
    "        callbacks = [early_stopping, reduce_lr]\n",
    "\n",
    "        history = model.fit(\n",
    "            x=X_train,\n",
    "            y=y_train,\n",
    "            batch_size=128,\n",
    "            epochs=40,\n",
    "            validation_split=0.3,\n",
    "            callbacks=callbacks,\n",
    "            verbose=2\n",
    "        ).history\n",
    "\n",
    "        # === EVALUATE PERFORMANCE ===\n",
    "        test_predictions = model.predict(X_test, verbose=0)\n",
    "        ModelPerformanceIndicators.save_pdf(test_predictions, y_test, model_name)\n",
    "\n",
    "        # === SAVE MODEL ===\n",
    "        model_filename = f'../models/{model_name}.keras'\n",
    "        model.save(model_filename)\n",
    "\n",
    "    finally:\n",
    "        # === CLEAN UP MEMORY ===\n",
    "        del X_train, X_test, y_train, y_test\n",
    "        del history, test_predictions\n",
    "        del model\n",
    "        gc.collect()  # Python garbage collection\n",
    "        tf.keras.backend.clear_session()  # Clears TensorFlow session/graph"
   ],
   "id": "580b2d1530573455",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-08-16T22:45:32.913709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for model_name in model_names:\n",
    "    print(f\"Training model: {model_name}\")\n",
    "    train_pipeline(model_name)"
   ],
   "id": "31d62a9aac6bdf60",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: I60-R5\n",
      "TRUE: 121686 images with shape (96, 180)\n",
      "FALSE: 99402 images with shape (96, 180)\n",
      "Balanced counts: {0: 99402, 1: 99402}\n",
      "Training set: 139162 samples\n",
      "Test set: 59642 samples\n",
      "Image shape: (96, 180)\n",
      "Epoch 1/40\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d487ba0f05032bcc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-macos)",
   "language": "python",
   "name": "tf-macos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
